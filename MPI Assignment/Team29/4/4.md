# Question 4: Inverse of a Matrix

## **Objective:**
The task is to compute the inverse of a given non-singular square matrix using distributed computing. The implementation utilizes MPI to parallelize the Gaussian elimination method.

## **Approach:**

### 1. **Matrix Augmentation:**
- The original matrix is augmented with an identity matrix, resulting in a matrix of size \(N \times 2N\). This setup allows us to apply row operations, that will eventually yield the inverse matrix.

### 2. **Scatter Operation:**
- The rows of the augmented matrix are distributed across multiple MPI processes using the `MPI_Scatter` function. Each process works on a subset of rows, enabling parallel processinng.

### 3. **Gaussian Elimination:**
- **Normalization:** For each pivot row, the process that owns the row normalizes it by dividing the entire row by the pivot element.
- **Broadcasting:** The normalized pivot row is broadcasted to all processes using `MPI_Bcast`.
- **Row Elimination:** Each process updates its local rows by eliminating the current column, driving the augmented matrix towards having the identity matrix on the left side and the inverse on the right.

### 4. **Gather Operation:**
- The rows of the matrix are gathered from all processes to the root process using `MPI_Gather`. The root process re-constructs the full inverse matrix from these rows.

### 5. **Output:**
- The root process extracts the right half of the augmented matrix (which now contains the inverse matrix) and outputs it, formatted to two decimal places as asked in the question.

## **MPI Functions Used:**
- `MPI_Init`, `MPI_Finalize`: Initialize and finalize the MPI environment.
- `MPI_Comm_rank`, `MPI_Comm_size`: Determine the rank of the process and the total number of processes.
- `MPI_Scatter`: Distribute parts of the augmented matrix to each process.
- `MPI_Bcast`: Broadcast the pivot row to all processes.
- `MPI_Gather`: Gather the processed rows from all processes to the root process.

### **Complexity Analysis:**

#### **1. Time Complexity:**
- **Serial Complexity:** The time complexity of Gaussian elimination in a serial implementation is \(O(N^3)\), where \(N\) is the number of rows (or columns) of the matrix.
- **Parallel Complexity:** When parallelized across \( p \) processes, the time complexity is reduced to approximately \(O\left(\frac{N^3}{p}\right)\). However, additional time is required for communication between processes, particularly for broadcasting pivot rows and gathering results, which adds an \(O(N^2)\) overhead per process.

#### **2. Message Complexity:**
- The message complexity is determined by the number of broadcast and gather operations:
  - **Broadcasts:** For each pivot row, a broadcast operation is performed, leading to a message complexity of \(O(N \log p)\) for the entire process, where \( p \) is the number of processes.
  - **Gathers:** After the row operations, the rows of the inverse matrix are gathered, adding another \(O(N \log p)\) complexity.
- **Total Message Complexity:** The overall message complexity is \(O(N \log p)\) for each phase (broadcasting and gathering), resulting in a total message complexity of \(O(N \log p)\).

#### **3. Space Requirements:**
- **Per Process:** Each process holds a portion of the augmented matrix of size \( \frac{N}{p} \times 2N \), requiring \(O\left(\frac{N^2}{p}\right)\) space.
- **Total Space:** The overall space requirement across all processes remains \(O(N^2)\), consistent with the size of the augmented matrix.


## **Performance Considerations:**
- The Gaussian elimination method is parallelized to improve performance on large matrices.
- Communication overhead is minimized by broadcasting only the necessary rows at each step.

## **Results:**
- The program successfully computes the inverse of a matrix in a distributed manner. The efficiency improves with the number of processes, particularly for large matrices.